{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Driver_RNN_PID_6_2   -  Version 6.2  - WORKING VERSION\n",
    "%pylab inline\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import csv\n",
    "import ast\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.core.debugger import Tracer; debug_here = Tracer()\n",
    "#from sklearn.datasets import load_iris\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Debugging\n",
    "dbg = True  # top level results\n",
    "dbg0 = True  # body response model\n",
    "dbg1 = False  # print print_frame\n",
    "dbg2 = False # set_traces in PID and MAIN\n",
    "dbg3 = False  # batches\n",
    "dbg4 = False  # step_j1\n",
    "dbg5 = False # end of n-loop - improvement ?\n",
    "dbg6 = False  # set trace at i = 2 in run_model\n",
    "dbg7 = False # create batch methods\n",
    "dbg8 = False # stop at NN stages\n",
    "dbg9 = False # Error at dot product\n",
    "dbg11 = False # check pump model\n",
    "dbg12 = False # deep in PID for error\n",
    "dbg13 = False # summary PID and set trace\n",
    "dbg14 = False # \n",
    "dbg15 = False # Shapes in NN\n",
    "dbg16 = False # advance PID\n",
    "dbg18 = False # improvement and accuracy parameters\n",
    "\n",
    "# flow control\n",
    "fluidLoss = False\n",
    "open_loop = True # train with NN updating PID parameters\n",
    "useExponentialDampingInPump = False\n",
    "train = True    # train versus closed loop runs\n",
    "test= False     # not yet implemented\n",
    "preview = True\n",
    "summary = True\n",
    "cutNeurons = True  # Just run the controller (PID, fuzzy, etc)\n",
    "\n",
    "sine = False # for testing Neural-Net\n",
    "rand = False  # for testing Neural-Net\n",
    "# If both sine and rand- are False, it uses actual train data\n",
    "\n",
    "'''Latest version:  Combines NN_for_PID and PID_test module codes.\n",
    "In this version, Neural Net serves as a \"judge\" using only its\n",
    "\"accuracy\" to discard batches if there is no improvement.  That is,\n",
    "we are not using the 3 output node results to directly input to the PID.\n",
    "\n",
    "Here we create batches in a different way, compatile with the Neural Net.\n",
    "Instead of inptting a file with many short sequences,\n",
    "we need to create one relatively long sequence and create rows for the PID\n",
    "in a sliding-window fashion.  See the new \"slicer\" below.\n",
    "\n",
    "Batches have 20 rows with 50 entries each to test the controller,\n",
    "the first 47 of each row are measured MAP (mean arterial pressure) values\n",
    "and the next 3 of which are the PID parameters Ki, Kd, and Kp; and\n",
    "the last is the PID's performance score in approaching a target MAP,\n",
    "which is the train-ing \"y\" fed to the NN, or \"score\".\n",
    "\n",
    "The controller ouptputs a new base batch.csv with each i-th loop,\n",
    "which after stepping slightly in each of the dimensions of the parameers,\n",
    "is then fed to the NN with judges the stepped parmeters (Kparms) by its own \n",
    "minibatch stochastic gradient descent method.  The accuracy and loss of a CNN or \n",
    "rmse and loss of a RNN;\n",
    "then modulate a gradient step for the controller parameters by slowing\n",
    "down or speeding up the steps.  We show that these feedback connections\n",
    "will not run the system into instability if parameters are held witn a\n",
    "certain Lipshitz-type constraint.   This is believed to prevent blow-up or \n",
    "zeroing of the NN, and walk-away by the essentially Newtonian step of the PID.\n",
    "This combinatoin is a semi-autonomous learner in the sense that both \n",
    "have access to an external \"true\" but that itself is noisy and\n",
    "there is in addition an unknown fluid loss.  The two, controller and NN\n",
    "together, are each adapting to noise and unknown fluid loss (if included).\n",
    "But the NN advises or \"judges\" from its point of view and allows the PID to\n",
    "make the decision about infusion with its (hopefully) improving parameters.\n",
    "The reason for choice of an RNN is that there is an unnkown internal state, the\n",
    "fluid loss, and the RNN will tend to perform like a Markov process in its search.\n",
    "Along these lines, another choice is a NN with short-term long-term memroy.\n",
    "\n",
    "Details\n",
    "An inner j1-loop determines a gradient, starting with the i-th batch\n",
    "and varying each parm in K, one at at time, by a slight amount to create deltaBatches.\n",
    "Thus we obtain \"coordinates\" of the gradient. This is crude but it works.\n",
    "\n",
    "We start by train-ing the NN - PID combination with two\n",
    "simple scenarios:\n",
    "(1) measured MAP remains about target MAP, for which PID should succeed.\n",
    "(2) measured MAP starts 16 mmHg below target, and PID should \"fail\" to reach target MAP.\n",
    "Each line has 47 MAP measurements, and these are used (each) to train the NN-PID.\n",
    "Since time step is 10 seconds, each profile constitutes 10 x 47 seconds of each dataset.\n",
    "\n",
    "See Driver_NN_PID_setup, which creates a dataset as well as a fluid losses file.\n",
    "\n",
    "Outer_loop:\n",
    "   runs the dataset through the PID,\n",
    "   computing a score for each row depending on degree of success\n",
    "   of PID in reaching a target MAP.  In the combined modules, \n",
    "   Driver_PID_NN, the neural net \n",
    "   runs batches of PID results seeking to improve the PID's\n",
    "   internal parameters.\n",
    "\n",
    "Conventions:\n",
    "i - indexes the outer loop which runs through a batch of profiles\n",
    "j1 - runs over the PID parm numbers, 1,...N, varying each as a batch is run\n",
    "k - \n",
    "m - the element of a batch row picked out for the PID\n",
    "n - cycles the PID - NN combination through batches\n",
    "\n",
    "We start with two simple scenarios: \n",
    "(1) measured MAP starts 16 mmHg below target, and PID should \"fail\" to reach target MAP,\n",
    "(2) measured MAP remains about target MAP, for which PID should succeed.\n",
    "\n",
    "There are other files ready to be used as batches, such as:\n",
    "  keras_3parms_glitches\n",
    "  keras_3parms_sinusoids\n",
    "\n",
    "Notes:\n",
    "the PID outputs are all appended to controls.csv for a record of PID progress.\n",
    "There is a lag, L, built in to the proportional component of the PID.\n",
    "The pump model depends on a delay and a resistance_factor usually near 1.\n",
    "The delay is assumed to be 1/10 second, i.e. one time step.\n",
    "\n",
    "u1 is an infusion rate in micro-gm/(kg x min).\n",
    "v1 is fluid loss (mL / sec) for one time interval (hemorrhage, urine, compartment, etc.)\n",
    "e1 is the error between target MAP and observed MAP (mmHg).\n",
    "'''\n",
    "### SORRY ABOUT SO MANY GLOBALS - IT'S MY CLUMSY STYLE!!!\n",
    "\n",
    "# Pump simulator parameters\n",
    "delay = 1   # assumed to be about one time step for now (10 sec.).\n",
    "resistance_factor = 0.95  # degree to which the pump can actually infuse.\n",
    "pumpNoiseLevel = 0.1\n",
    "useExponentialDampingInPump = True\n",
    "eMax = 20\n",
    "# this factor also includes the linear transfer function constant.\n",
    "\n",
    "# Response model global parameters -- NEED THESE FOR GLOBAL RECURSION.\n",
    "# THERE'S PROBABLY A WAY TO REMEMBER ACROSS CLASSES BUT I HAVEN'T FOUND.\n",
    "u1 = []\n",
    "v1 = []\n",
    "e1 = []\n",
    "VB3 = 0  # These are recursion parameters\n",
    "VB2 = 0\n",
    "VB1 = 0\n",
    "VB0 = 0\n",
    "Um1 = 0    # no reaction of model for first two steps\n",
    "Vm1 = 0\n",
    "Um2 = 0\n",
    "Vm2 = 0\n",
    "\n",
    "delta_K = []\n",
    "deriv_limit = 0.5  # limits the deriv-ative estimate in differential term of PID if used\n",
    "ave_score = 0\n",
    "num_scores = 0\n",
    "noise = 0.1       # noise * random  added to each coordinate in create batch_i_j\n",
    "epsilon = 0.05    # wiggle epsilon * random(0,1) if on a flat spot\n",
    "fraction = 0.01   # maximum step limited by fraction of K.  May need to be larger\n",
    "#                    for faster convergence of the PID parameters, K.\n",
    "out_f = \"\"        # output file names\n",
    "accuracy = 0      # current accuracy of current batch (if using CNN)\n",
    "K = [0,0,0]         # parameters for PID\n",
    "fileRoot = \"batch_\"  # prefix for batch file names\n",
    "i = 0             # loop batch numbering starts with 0\n",
    "\n",
    "dataSize = 400    # length of data, which is now only one row, Changes with data.\n",
    "batchSize = 20    # NN's batches same as baseBatches for PID ???\n",
    "trainingCycles = int(dataSize / batchSize)    # n1-loop range.\n",
    "trainingCycles = 5     ############ Debugging purposes\n",
    "M = batchSize     # PID's batch size - short name for batchSize\n",
    "Nparms = 3        # dimension of controller's parameters\n",
    "rowSize = 50      # earlier the \"T\" in NN_for_PID_4\n",
    "timeSteps = rowSize - Nparms    # we add Nparms parameters to the test sequence\n",
    "scores = [0 for i in range(M)]  # given by PID for how close to target MAP it reaches\n",
    "\n",
    "noise = 5         # noise level with uniform distribution, i.e. white noise\n",
    "quartile = 0.2    # null hypothesis for random white noise.  Lower is better.\n",
    "percentile = 0.0001 # for null hypothesis for Gaussian noise in NN accuracies\n",
    "\n",
    "targetMAP = 65    # desireable mean arterial blood pressure in pigs\n",
    "initialMAP = 60   # This is for a typical hypovolemic (hemorrhaged) MAP,\n",
    "                  # in mmHg typical for hypovolumic swine\n",
    "initialError = 5\n",
    "initialLoss = 0   # this gets the fluid loss model recursion started.\n",
    "initialInfusion = 60\n",
    "initialBleed = 7\n",
    "\n",
    "# Simulation parameters\n",
    "L = 3     # pure time delay, or lag in the im-pulse response, i.e. 10 sec.\n",
    "Kp = 1.2  # proportional gain (g kg-1 min-1) mmHg-1,  Might be as low as 0.3.\n",
    "Ki = 0.8  # inte-gral gain\n",
    "Kd = 0.1  # differential gain\n",
    "Ci = 2.1  # 1 mmHg rise in MAP proportional to 1 mL infusion\n",
    "Ci = 5    # ???\n",
    "Cm = 0    # Set Cm = 0 to eliminate fluid loss component entirely from PID control.\n",
    "Cp = 1    # Set Cp = 0 to eliminate the pump model.\n",
    "#    Set Cm = 0 and Cl= 1 both to eliminate the fluid loss file input and\n",
    "#    there could be a built-in updating of the fluid loss \"v\" for steady bleed.\n",
    "Mult = 1     # multiplier of basic time steps, 10 steps over 1 second\n",
    "TI = Mult * 10   # total simulation time\n",
    "Tc1 = 5   # time constant in the step response of \"proportional\".\n",
    "del_t = 1     # discrete time difference or sampling interval (10  sec)\n",
    "step = 1 # initializes the constrained gradient method in create_batch_i\n",
    "         # currently we are not using this but instead, create a new batch which\n",
    "         # computes a controller parameter gradient modulated by\n",
    "         # the accuracy achieved for the current batch.\n",
    "\n",
    "# Note: Set Cm = 0 to eliminate bleeding, i.e. fluid response VB0 from error:\n",
    "#      e1 = (targetMAP - Ci * infusion  +  Cm * VB0)\n",
    "\n",
    "# The following further parametrize the body response function due to Hahn et al. (slide 6)\n",
    "VB = 2500     # estimated blood volume for a 43 kg swine\n",
    "K = 1         # feedback gain\n",
    "alpha = 2.5   # fluid transfer rate between vascular and extravascular body \n",
    "              # compartments given a 900 mL bleed (Hahn)\n",
    "\n",
    "#------- Helper functions -------------\n",
    "\n",
    "def print_frame():\n",
    "    callerframerecord = inspect.stack()[1]    # 0 represents this line\n",
    "                                              # 1 represents line at caller\n",
    "    frame = callerframerecord[0]\n",
    "    info = inspect.getframeinfo(frame)\n",
    "    if(dbg1): \n",
    "        print(info.filename)                      # __FILE__\n",
    "        print(info.function)                      # __FUNCTION__\n",
    "        print(info.lineno)                        # __LINE__\n",
    "        print(\" \")\n",
    "    return info.lineno\n",
    "\n",
    "#--------------------------------------\n",
    "\n",
    "##-- Prepare for (revised) base batch in i-th direction (a deltaBatch)\n",
    "# Note, we keep previous batches for later optimization of hyperparameters\n",
    "#------------------------------------\n",
    "def create_batch_i_j (n, i, j1, bBatch, dBatch, K, noise):\n",
    "    # These are the Inner-loop's batches where PID parameters are\n",
    "    # varied one at a time.  The best gradient steps in each of\n",
    "    # the Nparm PID  parameters are extracted and recorded in the\n",
    "    # next base_batch for the NN.\n",
    "    success = 1\n",
    "    rows = []\n",
    "    \n",
    "    # REDUNDANT:  THIS IS ONE WAY TO DETERMINE A FILE SIZE\n",
    "    file = open(bBatch)\n",
    "    size = len(file.readlines()) # baseBatch from which deltaBatches are derived\n",
    "    if(size != M):\n",
    "        print(\"*** create_batch_i_j: on entry, bad batchSize = \",size,\" at i = \",i,\", j = \",j1,\", n = \",n)\n",
    "        print(print_frame())\n",
    "\n",
    "    # Read baseBatch here as rows and row\n",
    "    try:\n",
    "        with open(bBatch, 'r', newline='', encoding=\"utf8\") as inp:\n",
    "            csv_in = csv.reader(inp, delimiter=',')\n",
    "            count = 0\n",
    "            for row in csv_in:\n",
    "                rows.append(row)\n",
    "                count += 1    # IT WOULD BE JUST AS WELL TO USE len(rows)\n",
    "        if(count != M):\n",
    "            print(\"*** create_batch_i_j: reading bBatch, wrong batchSize = \",count,\" at i = \",i,\", n = \",n)\n",
    "            success = 0\n",
    "            print(print_frame())\n",
    "            if(dbg7): set_trace() #######################  count, rows\n",
    "    except OSError as err:\n",
    "        print(\"OS error: {0}\".format(err))\n",
    "        print(print_frame())\n",
    "        success = 2\n",
    "    except ValueError:\n",
    "        print(\"Could not convert data to an integer.\")\n",
    "        print(print_frame())\n",
    "        success = 3\n",
    "    except IOError:\n",
    "        print (\"File error with :\", baseBatch)\n",
    "        print(print_frame())\n",
    "        success = 4\n",
    "    except:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        print(print_frame())\n",
    "        success = 5\n",
    "        raise\n",
    "    finally:\n",
    "        inp.close()\n",
    "        if(dbg7): set_trace() ################ end of read loop create_batch_i\n",
    "        if (success != 1):\n",
    "            return success\n",
    "    if(dbg7): set_trace() ############### Now writing dBatch  \n",
    "    meas = []\n",
    "    try:\n",
    "        with open(dBatch, 'w', newline ='', encoding = \"utf8\") as outp:\n",
    "            #-write intermediate batch i_j with variation in the j-coordinate\n",
    "            csv_out = csv.writer(outp, delimiter=',')\n",
    "            k = 0\n",
    "            for k in range(0, M):\n",
    "                row = rows[k]\n",
    "                rowLength = len(row)\n",
    "                if (dbg7): print(\"create_batch_i_j: rowLength = \",rowLength,\", timeSteps = \",timeSteps)\n",
    "                #if(dbg7):\n",
    "                #    print(\"create_batch_i_j: rowLength = \",rowLength,\", timeSteps = \",timeSteps)\n",
    "                #    set_trace() ##################\n",
    "\n",
    "                # measurements at preceeding time steps\n",
    "                for m in range(0,timeSteps):\n",
    "                    val = row[m]\n",
    "                    meas.append(val)\n",
    "                # vary the j1-th PID parm\n",
    "                if (j1 == 0):\n",
    "                    # Note: this does not alter the K params\n",
    "                    Kp = K[0] + step   # this step is decreased as good value approached\n",
    "                    meas.append(Kp)\n",
    "                    meas.append(K[1])\n",
    "                    meas.append(K[2])\n",
    "                elif (j1 == 1):\n",
    "                    meas.append(K[0])\n",
    "                    Ki = K[1] + step\n",
    "                    meas.append(Ki)\n",
    "                    meas.append(K[2])\n",
    "                elif (j1 == 2):\n",
    "                    meas.append(K[1])\n",
    "                    meas.append(K[2])\n",
    "                    Kd = K[2] + step\n",
    "                    meas.append(Kd)\n",
    "                    # Note:  we don't append scores in these deltaBatches\n",
    "                    # scores are only appended in new batches created for the NN\n",
    "                else:\n",
    "                    print(\"*** Wrong j1 = \",j1,\", at i = \",i)\n",
    "                    success = 0\n",
    "                csv_out.writerow(meas)\n",
    "        if(dbg7): set_trace()  ##################### meas\n",
    "    except OSError as err:\n",
    "        print(\"OS error: {0}\".format(err))\n",
    "        print(print_frame())\n",
    "        success = 0\n",
    "    except ValueError:\n",
    "        print(\"Could not convert data to an integer.\")\n",
    "        print(print_frame())\n",
    "        success = 0\n",
    "    except IOError:\n",
    "        print (\"File error with :\", deltaBatch)\n",
    "        print(print_frame())\n",
    "        success = 0\n",
    "    except:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        print(print_frame())\n",
    "        success = 0\n",
    "        raise\n",
    "    finally:\n",
    "        outp.close()\n",
    "        if(dbg7): set_trace() ################ end of write loop create_batch_i\n",
    "        if (dbg):\n",
    "            print(\"create_batch_i_j successful\")\n",
    "    return success\n",
    "#- end create_batch_i_j\n",
    "#--------------------------------------\n",
    "\n",
    "def create_batch_i (n, i, bBatch, nBatch, K, scores, noise):\n",
    "    # These are the i-loop's batches where PID parameters have\n",
    "    # been optimized.  The best gradient steps in each of\n",
    "    # the N PID  parameters have been extracted and recorded in K\n",
    "    # for the next baseBatch for the NN.\n",
    "    # Note: We do include scores in newBatch for the NN to use.\n",
    "    success = 1\n",
    "    rows = []\n",
    "    if(dbg7): set_trace() ################### entry\n",
    "    file = open(bBatch)\n",
    "    lngF = len(file.readlines())\n",
    "    if(batchSize != lngF):\n",
    "        print(\"*** create_batch_i: wrong batchSize = \",lngF,\" at i = \",i,\", n = \",n)\n",
    "        print(print_frame())\n",
    "    try:\n",
    "        count = 0\n",
    "        with open(bBatch, 'r', newline='', encoding=\"utf8\") as inp:\n",
    "            csv_in = csv.reader(inp, delimiter=',')\n",
    "            for row in csv_in:\n",
    "                rows.append(row)\n",
    "                count += 1\n",
    "        if(dbg7): set_trace() ##########################\n",
    "\n",
    "        if(count != M):\n",
    "            print(\"*** create_batch_i: wrong batchSize = \",count,\" at i = \",i)\n",
    "            success = 0        \n",
    "    except OSError as err:\n",
    "        print(\"OS error at 278: {0}\".format(err))\n",
    "        print(print_frame())\n",
    "        success = 2\n",
    "    except ValueError:\n",
    "        print(\"Could not convert data to an integer.\")\n",
    "        print(print_frame())\n",
    "        success = 3\n",
    "    except IOError:\n",
    "        print (\"File error with :\", bBatch)\n",
    "        print(print_frame())\n",
    "        success = 4\n",
    "    except:\n",
    "        print(\"Unexpected error: \", sys.exc_info()[0])\n",
    "        print(print_frame())\n",
    "        success = 5\n",
    "        raise\n",
    "    finally:\n",
    "        inp.close()\n",
    "        if(dbg7): set_trace() ###################\n",
    "        if (success != 1): \n",
    "            return success\n",
    "    if(dbg2): set_trace() ################### Got passed reading in create_batch_i\n",
    "    meas = []\n",
    "    try:\n",
    "        with open(nBatch, 'w', newline ='', encoding = \"utf8\") as outp:\n",
    "            csv_out = csv.writer(outp, delimiter=',')\n",
    "            for k in range(0, M):\n",
    "                # Noise-up measurements at preceeding time steps -- WILL THIS ACCUMULATE ???\n",
    "                for m in range(0,timeSteps):\n",
    "                    val = float(rows[k][m]) + noise * random.uniform(0, 1)\n",
    "                    meas.append(val)\n",
    "                Kp = K[0] + noise * random.uniform(0, 1)\n",
    "                meas.append(Kp)\n",
    "                Ki = K[1] + noise * random.uniform(0, 1)\n",
    "                meas.append(Ki)\n",
    "                Kd = K[2] + noise * random.uniform(0, 1)\n",
    "                meas.append(Kd)\n",
    "                csv_out.writerow(meas)\n",
    "        if(dbg7): set_trace() ##################### meas\n",
    "    except OSError as err:\n",
    "        print(\"OS error at 317: {0}\".format(err))\n",
    "        print(print_frame())\n",
    "        success = 1\n",
    "    except ValueError:\n",
    "        print(\"Could not convert data to an integer.\")\n",
    "        print(print_frame())\n",
    "        success = 2\n",
    "    except IOError:\n",
    "        print (\"File error with :\", bBatch)\n",
    "        print(print_frame())\n",
    "        success = 3\n",
    "    except:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        print(print_frame())\n",
    "        success = 4\n",
    "        raise\n",
    "    finally:\n",
    "        outp.close()\n",
    "        if(dbg7): set_trace() ################ at end of create_batch_i.  Check meas\n",
    "        return success\n",
    "#- end reate_batch_i\n",
    "#--------------------------------------------\n",
    "# https://www.analyticsvidhya.com/blog/2019/01/fundamentals-deep-learning-recurrent-neural-networks-scratch-python/\n",
    "# Here the neural net as a helper function instead of a class, unlike PID.\n",
    "# May want to class-ify this in future for modularity.\n",
    "\n",
    "def NeuralNet (training):\n",
    "    if(dbg8): \n",
    "        print(\"-----Entering the NN\")\n",
    "        #set_trace() ####################### step on!\n",
    "    \n",
    "    # NN_for_PID_6\n",
    "    # See module for Resusc/Driver_NN_PID_6_xxx\n",
    "    #https://www.analyticsvidhya.com/blog/2019/01/fundamentals-deep-learning-recurrent-neural-networks-scratch-python/\n",
    "\n",
    "    '''This inputs single sequences of length, seq_num,\n",
    "    each of which is a \"minibatch\". \n",
    "    So the shape of the input data is:\n",
    "\n",
    "        (number_of_records x length_of_sequence x types_of_sequences)\n",
    "\n",
    "    Here, types_of_sequences is 1, because there is only one type of sequence.\n",
    "    On the other hand, the output will have only one value for each record,\n",
    "    the seq_num'th value in the input sequence. So it’s shape is:\n",
    "\n",
    "        (number_of_records x types_of_sequences)\n",
    "\n",
    "    where types_of_sequences is 1\n",
    "    '''\n",
    "    rows = []\n",
    "    X = []\n",
    "    Y = []\n",
    "    X_val = []\n",
    "    Y_val = []\n",
    "\n",
    "    '''Assuming an input file of dataSize = dataSize / 2 we can test with a sinewave\n",
    "    sin_wave = np.array([math.sin(x) for x in np.arange(dataSize)]).\n",
    "\n",
    "    If sin = False we will read a .csv file as output from \"RNNs\"\n",
    "    '''\n",
    "    seq_len = 50\n",
    "    if (sine == True):\n",
    "        sin_wave = np.array([math.sin(x/0.2) for x in np.arange(200)]) \n",
    "        + np.array([math.sin(x/0.5) for x in np.arange(200)])\n",
    "        dataSize = len(sin_wave) - seq_len  # set aside last 50 as validation data\n",
    "        for i in range(dataSize - seq_len):\n",
    "            X.append(sin_wave[i:i+seq_len])\n",
    "            Y.append(sin_wave[i+seq_len])\n",
    "        '''((dataSize, seq_len, 1), (numrecords, 1))\n",
    "        Create the validation data:\n",
    "        '''\n",
    "        for i in range(dataSize - seq_len, dataSize):\n",
    "            X_val.append(sin_wave[i:i+seq_len])\n",
    "            Y_val.append(sin_wave[i+seq_len])\n",
    "        plt.plot(sin_wave[:seq_len])\n",
    "    elif (rand == True):\n",
    "        wave = np.array([random.random() for x in np.arange(200)])\n",
    "        dataSize = len(wave) - seq_len  # set aside last 50 as validation data\n",
    "        for i in range(dataSize - seq_len):\n",
    "            X.append(wave[i:i+seq_len])\n",
    "            Y.append(wave[i+seq_len])\n",
    "        for i in range(dataSize - seq_len, dataSize):\n",
    "            X_val.append(wave[i:i+seq_len])\n",
    "            Y_val.append(wave[i+seq_len])\n",
    "        plt.plot(wave[:seq_len])\n",
    "    else:\n",
    "        waveStrings = np.array(training)\n",
    "        wave = waveStrings.astype(np.float)\n",
    "        dataSize = len(wave) - seq_len  # set aside last 50 as validation data\n",
    "        for i in range(dataSize - seq_len):\n",
    "            X.append(wave[i:i+seq_len])\n",
    "            Y.append(wave[i+seq_len])\n",
    "        for i in range(dataSize - seq_len, dataSize):\n",
    "            X_val.append(wave[i:i+seq_len])\n",
    "            Y_val.append(wave[i+seq_len])\n",
    "    if(dbg8): set_trace() ################# what is wave and waveStrings ???\n",
    "    if(dbg0): print(\"dataSize = \",dataSize)\n",
    "    # plt.plot(wave[:seq_len])\n",
    "    #if(dbg8): set_trace() ##################### at end of X, Y, X_val and Y_val packing\n",
    "    #==========================================\n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=2)\n",
    "\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=1)\n",
    "\n",
    "    # Print the shape of the data:\n",
    "    if(dbg15): print(\"X.shape \",X.shape,\", Y.shape \",Y.shape)\n",
    "\n",
    "    X_val = np.array(X_val)\n",
    "    X_val = np.expand_dims(X_val, axis=2)\n",
    "\n",
    "    Y_val = np.array(Y_val)\n",
    "    Y_val = np.expand_dims(Y_val, axis=1)\n",
    "    \n",
    "    if(dbg15): print(\"X_val.shape \",X_val.shape,\", Y_val.shape \",Y_val.shape)\n",
    "\n",
    "    '''Step 1: Create the Architecture for an RNN model\n",
    "    The model will take in the input sequence, \n",
    "    process it through a hidden layer of 100 units, and produce a single valued \n",
    "    output.'''\n",
    "\n",
    "    learning_rate = 0.0001    \n",
    "    nepoch = 6               \n",
    "    T = 50                   # length of sequence\n",
    "    hidden_dim = int(dataSize / 2)        \n",
    "    output_dim = 1\n",
    "\n",
    "    bptt_truncate = 5\n",
    "    min_clip_value = -10\n",
    "    max_clip_value = 10\n",
    "\n",
    "    #Define the weights of the network:\n",
    "    U = np.random.uniform(0, 1, (hidden_dim, T))\n",
    "    W = np.random.uniform(0, 1, (hidden_dim, hidden_dim))\n",
    "    V = np.random.uniform(0, 1, (output_dim, hidden_dim))\n",
    "\n",
    "    if(dbg15): print(\"Step 1 U.shape \",U.shape,\", W shape \",W.shape,\", V shape \",V.shape)\n",
    "    '''Here,\n",
    "    U the weight matrix for weights between input and hidden layers\n",
    "    V is the weight matrix for weights between hidden and output layers\n",
    "    W is the weight matrix for shared weights in the RNN layer (hidden layer)\n",
    "    Finally, we will define the activation function, sigmoid, to be used in the hidden layer:\n",
    "    '''\n",
    "    '''\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    '''\n",
    "    def sigmoid(x):\n",
    "        return np.where(x >= 0, \n",
    "            1 / (1 + np.exp(-x)), \n",
    "            np.exp(x) / (1 + np.exp(x)))\n",
    "    \n",
    "    '''Step 2: Train the Model\n",
    "    Now that we have defined our model, we can finally move on with training \n",
    "    it on sequence data. \n",
    "    We can subdivide the training process into smaller steps, namely:\n",
    "\n",
    "    Step 2.1 : Check the loss on training data\n",
    "    Step 2.1.1 : Forward Pass\n",
    "    Step 2.1.2 : Calculate Error\n",
    "    Step 2.2 : Check the loss on validation data\n",
    "    Step 2.2.1 : Forward Pass\n",
    "    Step 2.2.2 : Calculate Error\n",
    "    Step 2.3 : Start actual training\n",
    "    Step 2.3.1 : Forward Pass\n",
    "    Step 2.3.2 : Backpropagate Error\n",
    "    Step 2.3.3 : Update weights\n",
    "\n",
    "    We need to repeat these steps until convergence. If the model starts to overfit, \n",
    "    stop! Or simply pre-define the number of epochs.\n",
    "\n",
    "    Step 2.1: Check the loss on training data\n",
    "    We will do a forward pass through our RNN model and calculate the \n",
    "    squared error for the predictions for all records in order to get the loss value.'''\n",
    "\n",
    "    for epoch in range(nepoch):\n",
    "        # check loss on train\n",
    "        loss = 0.0\n",
    "\n",
    "        # do a forward pass to get prediction\n",
    "        for i in range(Y.shape[0]):\n",
    "            x, y = X[i], Y[i]                    # get input, output values of each record\n",
    "            prev_s = np.zeros((hidden_dim, 1))   # here, prev-s is the value of the previous \n",
    "            # activation of hidden layer; which is initialized as all zeroes\n",
    "            for t in range(T):\n",
    "                new_input = np.zeros(x.shape)    # we then do a forward pass for every timestep \n",
    "                # in the sequence\n",
    "                new_input[t] = x[t]              # for this, we define a single input for that timestep\n",
    "                mulu = np.dot(U, new_input)\n",
    "                mulw = np.dot(W, prev_s)\n",
    "                add = mulw + mulu\n",
    "                s = sigmoid(add)\n",
    "                mulv = np.dot(V, s)\n",
    "                prev_s = s\n",
    "                if(i == 0 and t == 0):\n",
    "                    if(dbg15): print(\"Step 2.1 mulu.shape \",mulu.shape,\", mulw.shape \",mulw.shape,\", mulv.shape \",mulv.shape)\n",
    "            # calculate error\n",
    "            #if(dbg8): set_trace() ##################### at error\n",
    "            loss_per_record = (float(y) - float(mulv))**2 / 2.0\n",
    "            #loss_per_record = (y - mulv)**2 / 2\n",
    "            loss += loss_per_record\n",
    "        loss = loss / float(y.shape[0])\n",
    "\n",
    "        '''Step 2.2: Check the loss on validation data\n",
    "        We will do the same thing for calculating the loss on validation data (in the same loop):\n",
    "        '''\n",
    "        # check loss on val\n",
    "        ##denom = y.shape[1] * 2\n",
    "        val_loss = 0.0\n",
    "        for i in range(Y_val.shape[0]):\n",
    "            x, y = X_val[i], Y_val[i]\n",
    "            prev_s = np.zeros((hidden_dim, 1))\n",
    "            for t in range(T):\n",
    "                new_input = np.zeros(x.shape)\n",
    "                new_input[t] = x[t]\n",
    "                mulu = np.dot(U, new_input)\n",
    "                mulw = np.dot(W, prev_s)\n",
    "                add = mulw + mulu\n",
    "                s = sigmoid(add)\n",
    "                mulv = np.dot(V, s)\n",
    "                prev_s = s\n",
    "                if(i == 0 and t == 0):\n",
    "                    if(dbg15): print(\"Step 2.2 mulu.shape \",mulu.shape,\", mulw.shape \",mulw.shape,\", mulv.shape \",mulv.shape)\n",
    "            # calculate error\n",
    "            loss_per_record = (float(y) - float(mulv))**2 / 2\n",
    "            val_loss += loss_per_record\n",
    "        val_loss = val_loss / float(y.shape[0])\n",
    "\n",
    "        print('Epoch: ', epoch + 1, ', Loss: ', loss, ', Val Loss: ', val_loss)\n",
    "\n",
    "        '''Should get the output:\n",
    "\n",
    "        Epoch:  1 , Loss:  [[101185.61756671]] , Val Loss:  [[50591.0340148]]\n",
    "        ...\n",
    "        (gets down to 16, 17 or something similar.  We stay at 30, 40)\n",
    "\n",
    "        Step 2.3: Start actual training\n",
    "        We will now start with the actual training of the network. In this, we will first do a forward pass to calculate the errors and a backward pass to calculate the gradients and update them. Let me show you these step-by-step so you can visualize how it works in your mind.\n",
    "\n",
    "        Step 2.3.1: Forward Pass\n",
    "        In the forward pass:\n",
    "\n",
    "        . We first multiply the input with the weights between input and hidden layers.\n",
    "        . Add this with the multiplication of weights in the RNN layer. This is because we \n",
    "        want to capture the knowledge of the previous timestep.\n",
    "        . Pass it through a sigmoid activation function.\n",
    "        . Multiply this with the weights between hidden and output layers.\n",
    "        . At the output layer, we have a linear activation of the values so we do not \n",
    "        explicitly pass the value through an activation layer\n",
    "        . Save the state at the current layer and also the state at the previous \n",
    "        timestep in a dictionary\n",
    "        Here is the code for doing a forward pass \n",
    "        (note that it is in continuation of the above loop):\n",
    "        '''\n",
    "        # train model\n",
    "        for i in range(Y.shape[0]):\n",
    "            x, y = X[i], Y[i]\n",
    "\n",
    "            layers = []\n",
    "            prev_s = np.zeros((hidden_dim, 1))\n",
    "            dU = np.zeros(U.shape)\n",
    "            dV = np.zeros(V.shape)\n",
    "            dW = np.zeros(W.shape)\n",
    "\n",
    "            dU_t = np.zeros(U.shape)\n",
    "            dV_t = np.zeros(V.shape)\n",
    "            dW_t = np.zeros(W.shape)\n",
    "\n",
    "            dU_i = np.zeros(U.shape)\n",
    "            dW_i = np.zeros(W.shape)\n",
    "\n",
    "            # forward pass\n",
    "            for t in range(T):\n",
    "                new_input = np.zeros(x.shape)\n",
    "                new_input[t] = x[t]\n",
    "                mulu = np.dot(U, new_input)\n",
    "                mulw = np.dot(W, prev_s)\n",
    "                add = mulw + mulu\n",
    "                s = sigmoid(add)\n",
    "                mulv = np.dot(V, s)\n",
    "                layers.append({'s':s, 'prev_s':prev_s})\n",
    "                prev_s = s\n",
    "                if(i == 0 and t == 0):\n",
    "                    if(dbg15): print(\"2.3.1 mulu.shape \",mulu.shape,\", mulw.shape \",mulw.shape,\", mulv.shape \",mulv.shape)\n",
    "            # calculate error\n",
    "            '''Step 2.3.2 : Backpropagate Error\n",
    "            Calculate the gradients at each layer, and \n",
    "            backpropagate the errors. Use truncated back propagation through time (TBPTT), \n",
    "            instead of vanilla backprop.\n",
    "            '''\n",
    "            # derivative of pred\n",
    "            dmulv = (float(mulv) - float(y))   # dmulv IS A VECTOR.\n",
    "\n",
    "            # backward pass\n",
    "            for t in range(T):\n",
    "                dV_t = np.dot(dmulv, np.transpose(layers[t]['s']))\n",
    "                dsv = np.dot(np.transpose(V), dmulv)\n",
    "\n",
    "                ds = dsv\n",
    "                dadd = add * (1 - add) * ds\n",
    "\n",
    "                dmulw = dadd * np.ones_like(mulw)\n",
    "\n",
    "                dprev_s = np.dot(np.transpose(W), dmulw)\n",
    "\n",
    "\n",
    "                for i in range(t-1, max(-1, t-bptt_truncate-1), -1):\n",
    "                    ds = dsv + dprev_s\n",
    "                    dadd = add * (1 - add) * ds\n",
    "\n",
    "                    dmulw = dadd * np.ones_like(mulw)\n",
    "                    dmulu = dadd * np.ones_like(mulu)\n",
    "\n",
    "                    dW_i = np.dot(W, layers[t]['prev_s'])\n",
    "                    dprev_s = np.dot(np.transpose(W), dmulw)\n",
    "\n",
    "                    new_input = np.zeros(x.shape)\n",
    "                    new_input[t] = x[t]\n",
    "                    dU_i = np.dot(U, new_input)\n",
    "                    dx = np.dot(np.transpose(U), dmulu)\n",
    "\n",
    "                    dU_t += dU_i\n",
    "                    dW_t += dW_i\n",
    "\n",
    "                dV += dV_t\n",
    "                dU += dU_t\n",
    "                dW += dW_t\n",
    "\n",
    "                '''Step 2.3.3 : Update weights\n",
    "                Lastly, we update the weights with the gradients of weights calculated. \n",
    "                One thing we have to keep in mind that the gradients tend to explode if you \n",
    "                don’t keep them in check.This is a fundamental issue in training neural \n",
    "                networks, called the exploding gradient problem. So we have to clamp them \n",
    "                in a range so that they dont explode. We can do it like this'''\n",
    "\n",
    "                if dU.max() > max_clip_value:\n",
    "                    dU[dU > max_clip_value] = max_clip_value\n",
    "                if dV.max() > max_clip_value:\n",
    "                    dV[dV > max_clip_value] = max_clip_value\n",
    "                if dW.max() > max_clip_value:\n",
    "                    dW[dW > max_clip_value] = max_clip_value\n",
    "\n",
    "                if dU.min() < min_clip_value:\n",
    "                    dU[dU < min_clip_value] = min_clip_value\n",
    "                if dV.min() < min_clip_value:\n",
    "                    dV[dV < min_clip_value] = min_clip_value\n",
    "                if dW.min() < min_clip_value:\n",
    "                    dW[dW < min_clip_value] = min_clip_value\n",
    "\n",
    "                # update\n",
    "                U -= learning_rate * dU\n",
    "                V -= learning_rate * dV\n",
    "                W -= learning_rate * dW\n",
    "                if(i == 0 and t == 0):\n",
    "                    if(dbg15): print(\"Step 2.3.3 U.shape \",U.shape,\", W shape \",W.shape,\", V shape \",V.shape)\n",
    "\n",
    "    '''Step 3: Get predictions\n",
    "    We will do a forward pass through the trained weights to get our predictions:'''\n",
    "\n",
    "    preds = []\n",
    "    for i in range(Y.shape[0]):\n",
    "        x, y = X[i], Y[i]\n",
    "        prev_s = np.zeros((hidden_dim, 1))\n",
    "        # Forward pass\n",
    "        for t in range(T):\n",
    "            mulu = np.dot(U, x)\n",
    "            mulw = np.dot(W, prev_s)\n",
    "            add = mulw + mulu\n",
    "            s = sigmoid(add)\n",
    "            mulv = np.dot(V, s)\n",
    "            prev_s = s\n",
    "            if(i == 0 and t == 0):\n",
    "                if(dbg15): print(\"Step 3  mulu.shape \",mulu.shape,\", mulw.shape \",mulw.shape,\", mulv.shape \",mulv.shape)\n",
    "        preds.append(mulv) \n",
    "    preds = np.array(preds)\n",
    "    \n",
    "    #if(dbg8): set_trace() ############################\n",
    "    #Plotting these predictions alongside the actual values:\n",
    "    plt.plot(preds[:, 0, 0], 'g')\n",
    "    plt.plot(Y[:, 0], 'r')\n",
    "    plt.show()\n",
    "\n",
    "    # Step 4. Validation\n",
    "\n",
    "    preds = []\n",
    "    mulu = 1.0\n",
    "    mulw = 2.0\n",
    "    mulv = 3.0\n",
    "    for i in range(Y_val.shape[0]):\n",
    "        x, y = X_val[i], Y_val[i]\n",
    "        prev_s = np.zeros((hidden_dim, 1))\n",
    "        # For each time step...\n",
    "        for t in range(T):\n",
    "            mulu = np.dot(U, x)\n",
    "            mulw = np.dot(W, prev_s)\n",
    "            add = mulw + mulu\n",
    "            s = sigmoid(add)\n",
    "            mulv = np.dot(V, s)\n",
    "            prev_s = s\n",
    "            if(i == 0 and t == 0):\n",
    "                if(dbg15): print(\"Step 4  mulu.shape \",mulu.shape,\", mulw.shape \",mulw.shape,\", mulv.shape \",mulv.shape)\n",
    "        preds.append(mulv)   \n",
    "    preds = np.array(preds)\n",
    "    #if(dbg8): set_trace() ############################\n",
    "        \n",
    "    plt.plot(preds[:, 0, 0], 'g')\n",
    "    plt.plot(Y_val[:, 0], 'r')\n",
    "    plt.show()\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    max_val = 1\n",
    "    rmse = math.sqrt(mean_squared_error(Y_val[:, 0] * max_val, preds[:, 0, 0] * max_val))\n",
    "    if(dbg0): print(\"rmse = \",rmse,\", loss = \",loss)\n",
    "\n",
    "    return loss, rmse\n",
    "    # end NN\n",
    "#===========================================================================\n",
    "\n",
    "#===================================\n",
    "# This class is from from PID_test -- tests PID_incremental_controller\n",
    "# if further module testing is needed for the PID.\n",
    "# These results were obtained for rabbits with norepinephrine infusion.\n",
    "# May have to tinker with the constants a bit for other animals and infusants.\n",
    "'''\n",
    "u is an infusion rate in micro-gm/(kg x min).\n",
    "v is fluid loss (mL / sec) for one time interval (hemorrhage, urine, compartment, etc.)\n",
    "e is the error between target MAP and observed MAP (mmHg).'''\n",
    "\n",
    "# Model_increment is a time-stepped linear body response model to infusion and fluid loss.\n",
    "# It follows: Bighamian et al. A Lumped-Parameter Blood Volume Model.\n",
    "\n",
    "# T is sampling interval\n",
    "# VB is the response to u and v as change in blood volume output normalized by VB0\n",
    "# To convert this to control \"delta V\", we have delta V = VB * (VB0 - VB1),\n",
    "# where VB is blood volume estimate,\n",
    "# K is feedback gain (specifying the speed of ﬂuid shift),\n",
    "# alpha is the ratio between the volume changes in the intravascular and interstitial ﬂuids.\n",
    "#\n",
    "# The controller retains the 1/(1+α) fraction of the input ﬂuid volume in the intravascular compartment \n",
    "# while shifting the remaining α/(1 + α) fraction to the interstitial compartment in the steady state. \n",
    "# The ﬂuid shift from the intravascular to interstitial compartment (q) acts as feedback control to steer VB \n",
    "# to the target change in blood volume (rB) [Hahn].\n",
    "\n",
    "# u is total infusant  OR incremental?\n",
    "# i is time step counter, each step being T\n",
    "\n",
    "#--- Helper functions\n",
    "# Infusion pump transfer function\n",
    "# We can give pump some history in the error array\n",
    "def pump_model(error, infusion, delay, pumpNoiseLevel):\n",
    "    if(dbg11): set_trace()\n",
    "    # Step thru pump model\n",
    "    if(useExponentialDampingInPump):\n",
    "        if (len(error) >= delay):\n",
    "            exponent = -(error[-delay] / eMax)\n",
    "            if (exponent < 30 and exponent > -30):\n",
    "                slowdown = 1 - math.exp(exponent)  # prevents going too fast as error nears 0\n",
    "                if(dbg): print(\"slowdown = \",slowdown)\n",
    "            else:\n",
    "                slowdown = 1\n",
    "                print(\"*** pump exponent too large or too small\")\n",
    "            result = infusion * slowdown * resistance_factor\n",
    "            return result + pumpNoiseLevel * random.uniform(0, 1)\n",
    "        else:\n",
    "            return infusion  + pumpNoiseLevel * random.uniform(0, 1)\n",
    "    else:\n",
    "        result = infusion * resistance_factor  + pumpNoiseLevel * random.uniform(0, 1)\n",
    "        return result\n",
    "    \n",
    "# Impulse response, included in the \"proportional\" component\n",
    "def impulse (i):\n",
    "    val = 0\n",
    "    val1 = 0\n",
    "    if (i < Tc and i >= L):\n",
    "        val1 = math.exp(-((i - L) / Tc))\n",
    "        val = (Ki / Tc) * val1                                 # Kashihara equ (1)\n",
    "    if(dbg16):\n",
    "        print(\" - impulse: val = \",val,\", exp = \",val1,\", i-L = \",i - L,\", i = \",i,)\n",
    "    return val\n",
    "\n",
    "# e1 is the \"error\" between target MAP and measured MAP.\n",
    "# Calling sequence:  self.i - Tc, self.i - 1, self.i, e1\n",
    "def integral (t0, tN, i, e):\n",
    "    sum = 0\n",
    "    len_e = len(e)\n",
    "    if(dbg16): print(\" - inte-gral: len_e = \",len_e)\n",
    "    for j in range(t0, tN - 1):\n",
    "        if(dbg16): \n",
    "            print(\"Start integral: t0 = \",t0,\", tN - 1 = \",tN - 1,\", j = \",j,\", i-j = \",i - j,\", len_e = \",len_e)\n",
    "            #set_trace() ######### inte-gral loop\n",
    "        if ( ((i - j) > -1) and (j <= len_e)):\n",
    "            if(dbg16): print(\"integral before impulse: e[j] = \",e[j],\" del_t = \",del_t)\n",
    "            sum = sum + impulse(i - j) * e[j] * del_t\n",
    "            if(dbg16): print(\"integral sum after impulse: sum = \",sum)\n",
    "        return sum\n",
    "\n",
    "def first_deriv(h1, i):   # WE'VE GOT TO FILTER THIS A LOT\n",
    "    #hd = [0] * 200\n",
    "    global hd\n",
    "    derType = 0\n",
    "    diff = 0\n",
    "    deriv = 0\n",
    "    lenh1 = len(h1)\n",
    "    if(dbg23):\n",
    "        print(\"i = \",i,\", h1 : \",h1)\n",
    "\n",
    "    if(i >= 4 and lenh1 > 3):\n",
    "        hd.append((h1[-4] + h1[-3] + h1[-2] + h1[-1]) / 4)\n",
    "    elif(i >= 3 and lenh1 > 2):\n",
    "        hd.append((h1[-3] + h1[-2] + h1[-1]) / 3)\n",
    "    elif(i >= 2 and lenh1 > 1):\n",
    "        hd.append((h1[-2] + h1[-1]) / 2)\n",
    "    elif(i >= 1 and lenh1 > 0):\n",
    "        hd.append(h1[-1])  # THIS MAY BE JERKY\n",
    "    else:\n",
    "        hd.append(0)\n",
    "    if(dbg3): set_trace() #############       \n",
    "    len_h = len(hd)\n",
    "    if ((i < Tc1 - 1) and (i > 2) and (i <= len_h) and (len_h > 2)):\n",
    "        derType = 2\n",
    "        diff = hd[i-1] - hd[i-3]\n",
    "        deriv = diff / 2 * del_t\n",
    "    elif ((i < Tc1 - 1) and (i > 1) and (i <= len_h) and (len_h > 1)):\n",
    "        derType = 1\n",
    "        diff = hd[i-1] - hd[i-2]\n",
    "        deriv = diff / del_t\n",
    "    else:\n",
    "        dertype = 0\n",
    "        deriv = 0\n",
    "    if(dbg23):\n",
    "        print(\"---first deriv: len h = \",len_h,\", i = \",i,\", derivative Type = \",derType)\n",
    "        print(\" - diff = \",diff,\", hd = \",hd)\n",
    "        print(\" - Tc - 1 = \",Tc1-1,\", returning deriv = \",deriv)\n",
    "    \n",
    "    if(dbg23): set_trace() #############\n",
    "    return deriv\n",
    "\n",
    "# Fill the losses list, v1, with the second column.\n",
    "# First column is just a line number, skipped.\n",
    "def get_losses(losses_f):\n",
    "    if(dbg): print(\"-getLosses entry\")\n",
    "    count = 0\n",
    "    try:\n",
    "        with open(losses_f, 'r', newline = '', encoding = \"utf8\") as inp:\n",
    "            csv_in = csv.reader(inp, delimiter=',')\n",
    "            count += 1\n",
    "            for row in csv_in:\n",
    "                v1.append(row[1])\n",
    "        if(dbg):\n",
    "            print(\"-getLosses returned \",count,\" lines from \",losses_f)\n",
    "    except csv.Error as e1:\n",
    "        sys.exit('file {}, line {}: {}'.format(filename, reader.line_num, e1))\n",
    "        print(\"***getLosses Fluid losses or bleeding file couldn't be openned\")\n",
    "        print(print_frame())\n",
    "    except IOError:\n",
    "        print (\"***getLosses file error with :\",losses_f)\n",
    "        print(print_frame())\n",
    "    except:\n",
    "        print(\"***getLosses: Unexpected error:\", sys.exc_info()[0])\n",
    "        print(print_frame())\n",
    "        raise\n",
    "    finally:\n",
    "        inp.close()\n",
    "        print(\"-getLosses: Fluids test opened normally.  398\")\n",
    "        \n",
    "#==============================\n",
    "class PID:\n",
    "    #PID(i,j1,k,m,\"bleeding.csv\",row,\"controls.csv\",finalMAP,VB=2500,K0=2.1,alpha=0.5)\n",
    "    def __init__(self,i,j,k,m,losses_f,MAP,control_f,finalMAP,VB=2500,K0=2.1,alpha=0.5):\n",
    "        self.VB = VB\n",
    "        self.K0 = K0\n",
    "        self.alpha = alpha\n",
    "        self.losses_f = losses_f\n",
    "        self.MAP  = MAP            # this is usually a row of data from a batch\n",
    "        self.control_f = control_f\n",
    "        self.finalMAP = finalMAP\n",
    "        self.control = \"\"\n",
    "        self.total_outputs = 0\n",
    "        self.count = 0  #\n",
    "        self.i = i  # time step number (now row number in this test)\n",
    "        self.j = j  # controller param number = 0, 1, 2 (not used)\n",
    "        self.k = k  # controller now reading row k\n",
    "        self.m = m  # at row element m\n",
    "\n",
    "    def run_model(self):\n",
    "        global VB0\n",
    "        global VB1\n",
    "        global VB2\n",
    "        global VB3\n",
    "        global Um1\n",
    "        global Um2\n",
    "        global Vm1\n",
    "        global Vm2\n",
    "        \n",
    "        #if(dbg0): set_trace() ############## run model entry\n",
    "        if(self.i > 2):\n",
    "            sys.exit() ###############\n",
    "        try:\n",
    "            A = 0 # It's easier to debug large formulas in pieces\n",
    "            B = 0\n",
    "            C = 0\n",
    "            D = 0\n",
    "            E = 0\n",
    "            if self.i < 2:  # recursion begins at second time for volume response \n",
    "                VB3 = 0  \n",
    "                VB2 = 0\n",
    "                VB1 = 0\n",
    "                VB0 = 0\n",
    "\n",
    "                Um1 = initialInfusion\n",
    "                Vm1 = initialBleed\n",
    "\n",
    "                Um2 = 0 \n",
    "                Vm2 = 0\n",
    "            else:\n",
    "                if(dbg0):\n",
    "                    print(\" \")\n",
    "                    print(\"--- Model: preparing model at i = \",self.i)\n",
    "                    #print(\"Um2 = \",Um2,\", Um1 = \",Um1)\n",
    "                    #print(\"Vm2 = \",Vm2,\", Vm1 = \",Vm1)\n",
    "                    \n",
    "                if(dbg6): set_trace() ################## \n",
    "                A = 2 * VB1 - VB2\n",
    "                B = - self.K0 * (VB1 - VB2)\n",
    "                C = (self.K0 / self.VB) * ((float(Um1) - float(Um2)) - (float(Vm1) - float(Vm2)))\n",
    "                D = (self.K0 * self.K0 / (self.VB * (1 + self.alpha))) * (float(Um2) - float(Vm2))\n",
    "                VB0 = (A + B + C + D)\n",
    "\n",
    "                # keep short memory for recursion\n",
    "                Um2 = Um1\n",
    "                if (len(u1) > 1):\n",
    "                    Um1 = u1[-1]\n",
    "                else:\n",
    "                    Um1 =0\n",
    "                Vm2 = Vm1\n",
    "                if (len(v1) > 1):\n",
    "                    Vm1 = v1[-1]\n",
    "                else:\n",
    "                    Vm1 = 0\n",
    "                VB3 = VB2\n",
    "                VB2 = VB1\n",
    "                # Careful, this model tends to oscillate with period 2\n",
    "                VB1 = (VB0 + VB1) / 2\n",
    "\n",
    "                if(dbg0):\n",
    "                    print(\" \")\n",
    "                    print(\"----run_model: at i = \",str(self.i),\" m = \",str(self.m))\n",
    "                    print(\"-run_model: A = \",A,\", B = \",B,\", C = \",C,\", D = \",D)\n",
    "                    print(\"VB3 = \",VB3,\", VB2 = \",VB2,\", VB1 = \",VB1,\", VB0 = \",VB0)\n",
    "                    print(\"Um2 = \",Um2,\", Um1 = \",Um1)\n",
    "                    print(\"Vm2 = \",Vm2,\", Vm1 = \",Vm1)\n",
    "        except OSError as err:\n",
    "            print(\"OS error : {0}\".format(err))\n",
    "            print(print_frame())\n",
    "        except ValueError:\n",
    "            print(\"Could not convert data to an integer.\")\n",
    "            print(print_frame())\n",
    "        except IOError:\n",
    "            print (\"File error with : \", baseBatch)\n",
    "            print(print_frame())\n",
    "        except:\n",
    "            print(\"Unexpected error  run model :\", sys.exc_info()[0])\n",
    "            print(print_frame())\n",
    "        finally:\n",
    "            if(dbg0): print(\"Finished run model\")\n",
    "            return\n",
    "                \n",
    "    def advance_PID(self):\n",
    "        A1 = 0 # It's easier to debug large formulas in pieces\n",
    "        B1 = 0\n",
    "        C1 = 0\n",
    "        D1 = 0\n",
    "        E1 = 0\n",
    "        \n",
    "        ma = \"0\"\n",
    "        ua = \"0\"\n",
    "        va = \"0\"\n",
    "        ea = \"0\"\n",
    "\n",
    "        if (dbg16 and self.m == 1): \n",
    "            print(\"---------------advance_PID: at entry, i = \",self.i,\", m = \",self.m)\n",
    "            #set_trace() ############## advance PID entry CLICK ONE MORE TIME\n",
    "        try:\n",
    "            out_f = open(self.control_f,\"a\",newline='',encoding=\"utf8\")\n",
    "            csv_out = csv.writer(out_f, delimiter=',')\n",
    "            controller = \" \"\n",
    "            header = \"  loop,   infusion,   loss,    error\"\n",
    "            out_f.write(header)\n",
    "\n",
    "            # start the filter as best can for first time step\n",
    "            if (self.m != None and self.m < 1):\n",
    "                e1.append(initialError)\n",
    "                v1.append(initialLoss)\n",
    "                \n",
    "                # Here is the pump simulation\n",
    "                if (Cp == 1):\n",
    "                    result = pump_model(e1, initialInfusion, delay, pumpNoiseLevel)\n",
    "                else:\n",
    "                    result = initialInfusion\n",
    "                \n",
    "                u1.append(result)   # initial infusion. (This accumulates total)\n",
    "                controller = str(self.i) + \",\" + str(self.m) + \",\" + str(u1[0]) + \",\" + str(v1[0]) + \",\" + str(e1[0])\n",
    "                out_f.write(controller)\n",
    "                self.finalMAP = initialMAP\n",
    "            \n",
    "            # subsequent time steps\n",
    "            elif(self.m != None):\n",
    "                # -------- PID control function\n",
    "                if (dbg16): print(\"--advance_PID: at i = \",self.i,\", m = \",self.m)\n",
    "                # (Easier debugging to break up formula into these 4 parts)\n",
    "                if(dbg16): set_trace() ####################### start details of advance PID\n",
    "\n",
    "                # proportional term\n",
    "                B1 = 0\n",
    "                val3 = 0\n",
    "                lengE = len(e1)\n",
    "                if (self.m > 0 & self.m - 1 < lengE):\n",
    "                    val3 = impulse (e1[self.m - 1])\n",
    "                    B1 = Kp * val3\n",
    "                else:\n",
    "                    B1 = 0\n",
    "                    if(dbg): \n",
    "                        print(\"--- self.m - 1 getting ahead of e1 at B1\")\n",
    "                        print(print_frame())\n",
    "                if(dbg16): \n",
    "                    print(\"proportional, B1 = \",B1,\", Kp = \",Kp,\", impulse = \",val3,\", m = \",self.m,\", lengE = \",lengE)\n",
    "                \n",
    "                # integration term\n",
    "                C1 = 0\n",
    "                if (self.m >= Tc):  # inte-gral goes back T - 1 steps\n",
    "                    ## def integral (t0, tN, i, e):\n",
    "                    C1 = Ki * integral(self.m - Tc, self.m - 1, self.m, e1)\n",
    "                if(dbg16 and self.m >= Tc):\n",
    "                    print(\"integration: C1 = \",C1,\", Ki = \",Ki,\", e 1 = \",e1,\", Tc = \",Tc,\", m = \",self.m)\n",
    "                    #set_trace() ######### after integral C1, Ki, e1, Tc\n",
    "                    \n",
    "                D1 = 0\n",
    "                # differential term\n",
    "                if (self.m > 1):\n",
    "                    deriv = first_deriv (e1, self.m - 1)\n",
    "                    if (abs(deriv) <= deriv_limit):\n",
    "                        D1 = Kd + deriv\n",
    "                    else:\n",
    "                        if(dbg): print(\"-- advance_PID: Had to truncate deriv \",deriv)\n",
    "                        if (deriv > deriv_limit):\n",
    "                            D1 = Kd * 0.5\n",
    "                        elif (deriv < 0.5):\n",
    "                            D1 = - Kd * 0.5\n",
    "                if(dbg16 and self.m > 1):\n",
    "                    print(\"differential: D1 = \",D1,\", Kd = \",Kd,\", e 1 = \",e1,\", deriv = \",deriv,\", limit = \",deriv_limit,\", m = \",self.m)\n",
    "                    \n",
    "                infusion = B1 + C1 + D1   # increment of infusion recommended\n",
    "                \n",
    "                if (Cp == 1):\n",
    "                    pump_out = pump_model(e1, infusion, delay, pumpNoiseLevel)  # pump transfer function\n",
    "                    # pump may decrease the infusion rate if it is too fast, depending on e1\n",
    "                else:\n",
    "                    pump_out = infusion\n",
    "                if(dbg16): print(\"pump out = \",pump_out,\", m = \",self.m)\n",
    "                    \n",
    "                lng_u1 = len(u1)\n",
    "                if (self.m - 1 < lng_u1):\n",
    "                    tot_infusion = pump_out +  u1[self.m - 1]\n",
    "                else:\n",
    "                    if(dbg): \n",
    "                        print(\"--- self.m - 1 getting ahead of u1 at tot_infusion\")\n",
    "                        print(print_frame())\n",
    "                    tot_infusion = pump_out\n",
    "                \n",
    "                u1.append(tot_infusion) # total infusions up to and including m-th time step\n",
    "                if(dbg16): print(\"tot infusion = \",tot_infusion,\", m = \",self.m)\n",
    "    \n",
    "                lnMAP = len(self.MAP)\n",
    "                if (self.m < lnMAP):\n",
    "                    er = float(self.MAP[self.m]) - self.finalMAP\n",
    "                    e1.append (er)\n",
    "                else:\n",
    "                    er = initialError\n",
    "                    e1.append (er)\n",
    "                    if(dbg): \n",
    "                        print(\"--- self.m - 1 getting ahead of u1\")\n",
    "                        print(print_frame())\n",
    "                    \n",
    "                # Here is the combination: total infusion - fluid loss (if Cm > 0)\n",
    "                self.finalMAP = Ci * tot_infusion  -  Cm * VB0\n",
    "                if(dbg16): print(\"finalMAP = \",self.finalMAP,\", Ci = \",Ci,\", Cm = \",Cm,\", m = \",self.m)\n",
    "    \n",
    "                ln_u = len(u1)\n",
    "                ln_v = len(v1) # be careful, this array may not be used if Cm == 0\n",
    "                ln_e = len(e1)\n",
    "                \n",
    "                # Controller output parameters\n",
    "                if (self.m < ln_u and self.m < ln_e):\n",
    "                    ma = str(self.m)\n",
    "                    ua = str(u1[self.m])\n",
    "                    if (Cm > 0):\n",
    "                        va = str(v1[self.m])\n",
    "                    else:\n",
    "                        va = \"None\"\n",
    "                    ea = str(e1[self.m])\n",
    "                else:\n",
    "                    ma = \"0\"\n",
    "                    ua = \"0\"\n",
    "                    if (Cm > 0):\n",
    "                        va = \"0\"\n",
    "                    else:\n",
    "                        va = \"Wrong\"\n",
    "                    ea = \"0\"\n",
    "                    if(dbg2):\n",
    "                        print(\"-advance_PID: ln_u = \",ln_u,\", ln_e = \",ln_e,\". Can't compute self.control\")\n",
    "                if(dbg16):\n",
    "                        print(\"advance_PID: ln_u = \",ln_u,\", ln_v = \",ln_v,\", ln_e = \",ln_e,\", m = \",self.m)\n",
    "                        #set_trace() ############ \n",
    "\n",
    "                if(dbg13):\n",
    "                    print(\" \")\n",
    "                    print(\"---advance_PID  A1 = \",A1,\", B1 = \",B1,\", C1 = \",C1,\", D1 = \",D1)\n",
    "                    print(\" - e 1 is : \",e1[-self.m:-1])\n",
    "                    print(\" - infusion : \",infusion)\n",
    "                    print(\" - pump_out  : \",pump_out)\n",
    "                    print(\" - u 1 is now : \",u1[-self.m:-1])\n",
    "                    if(fluidLoss == True & ln_v > self.m):\n",
    "                        print(\" - v 1 is now : \",v1[-self.m:-1])\n",
    "                    print(\" - m = \",ma,\", u1 = \",ua,\", v1 = \",va,\", e1 = \",ea)\n",
    "                if(dbg13): set_trace() ######################## end of advance PID\n",
    "                    \n",
    "                self.control = ma + \", \" + ua + \", \" + va + \", \" + ea   # keeps record of control\n",
    "\n",
    "                # Note: this is the file needed in a trained, closed-loop control.\n",
    "                # Be sure to delete this file before starting a closed-loop run.\n",
    "                out_f.write(self.control)\n",
    "                \n",
    "                self.total_outputs += 1\n",
    "\n",
    "                #print(\"-advance_PID: updating i to \",self.i)\n",
    "                if(dbg16): print(\"-advance_PID returning error \",e1[-self.m:-1])\n",
    "            else:\n",
    "                print(\"*** self.i  is a None!\")\n",
    "        except OSError as err:\n",
    "            print(\"OS error: {0}\".format(err))\n",
    "            print(print_frame())\n",
    "            raise\n",
    "        except ValueError:\n",
    "            print(\"Could not convert data to an integer.\")\n",
    "            print(print_frame())\n",
    "            raise\n",
    "        except IOError:\n",
    "            print (\"File error with :\", self.control_f)\n",
    "            print(print_frame())\n",
    "            raise\n",
    "        except:\n",
    "            print(\"Error in advance PID: \", sys.exc_info()[0])\n",
    "            print(print_frame())\n",
    "            raise\n",
    "        finally:\n",
    "            out_f.close()\n",
    "            if(dbg1): print('PID_incremental_controller iteration completed normally.')\n",
    "            return\n",
    "        # End advance_PID\n",
    "        \n",
    "        def get_average_error(self):\n",
    "            leng = len(e1)\n",
    "            for i in range(0,leng):\n",
    "                e_sum = e1[i]\n",
    "            if (leng > 0):\n",
    "                return e_sum / leng\n",
    "            else:\n",
    "                print(\"*** error list has 0 length\")\n",
    "                return 0\n",
    "        \n",
    "        def get_control(self):\n",
    "            return self.control\n",
    "#-- End PID controller class\n",
    "\n",
    "#------------------------------------\n",
    "# Gradient descent process\n",
    "# acc and acc_prev are using NN's accuracies in this version\n",
    "def step_j_th_parm (acc, acc_prev, j1, K, epsilon):\n",
    "    if(dbg12): set_trace() ############## entry step_j_th_parm\n",
    "    try:\n",
    "        # Limits change to P % of acc_m1, e.g. 1 %.\n",
    "        change = 0\n",
    "\n",
    "        delta_acc = acc - acc_prev\n",
    "\n",
    "        # Remember, Newton's method is\n",
    "        #      X(n+1)  =  X(n)  -  f(X(n) / f'(X(n))\n",
    "        # The crude gradient descent method uses this method in all coordinates. \n",
    "        if (delta_acc > epsilon and acc > epsilon):\n",
    "             change  =  -  acc / delta_acc\n",
    "        elif (delta_acc > epsilon and acc < epsilon):\n",
    "            # Note that a flat spot occurs at f(Xn) = 0, f'(Xn) != 0.\n",
    "            # As long as this is not the last base batch, we randomize the \n",
    "            # batch and keep moving!\n",
    "            change = 0\n",
    "        else:\n",
    "            #  An indeterminant spot occurs when both f and f' = 0 closely.\n",
    "            #  We should kick things off a flat spot randomly some % of f\n",
    "            change = epsilon * random.uniform(0, 1)\n",
    "        # change, then, is the Neutonian step.  Very crude.  Instead we step\n",
    "        # with an arithmetic progression which begins more slowly.\n",
    "        \n",
    "        if (change > K[j1] * fraction):\n",
    "            change = K[j1] * fraction\n",
    "\n",
    "        Klast = K[j1]\n",
    "        # check the derivation in code document for this formula\n",
    "        if(acc_prev != 0):\n",
    "            K[j1] = Klast + K[j1] * change / acc_prev\n",
    "        \n",
    "        if(dbg4):\n",
    "            print(\"--- step_j_th_parm:\")\n",
    "            print(\" - K[\",j1,\"] = \",K[j1],\", Klast = \",Klast)\n",
    "            print(\" - change = \",change,\", acc = \",acc,\", accPrev = \",accPrev)\n",
    "            set_trace() ##################\n",
    "\n",
    "        # Note, scores and accuracy may continue to improve even if we keep the\n",
    "        #s ame batch data randomized somewhat.\n",
    "        # Note also that K does accumulate over j, so after 3 inner loops, all\n",
    "        # coordinaes of K are updated.\n",
    "    except OSError as err:\n",
    "        print(\"OS error : {0}\".format(err))\n",
    "        print(print_frame())\n",
    "        raise\n",
    "    except ValueError:\n",
    "        print(\"Could not convert data to an integer.\")\n",
    "        print(print_frame())\n",
    "        raise\n",
    "    except IOError:\n",
    "        print (\"File error with : \", baseBatch)\n",
    "        print(print_frame())\n",
    "        raise\n",
    "    except:\n",
    "        print(\"Unexpected error step_j_th_parm:\", sys.exc_info()[0])\n",
    "        print(print_frame())\n",
    "        raise\n",
    "    finally:\n",
    "        if(dbg2): print(\"ending step-j-th_parm\")\n",
    "    return K\n",
    "# end step_j_th_parm\n",
    "#------------------------------------\n",
    "\n",
    "# Here is where the neural network gets to \"vote\" on keeping settings of K.\n",
    "# acc is NN's curent accuracy; acc_last is last accuracy achieved; acc_prev next to last.\n",
    "def improvement (acc, acc_last, acc_prev, Gaussian_noise):\n",
    "    # After running the last base batch, \n",
    "    # THIS NEEDS TO ACCUMULATE A GAUSSIAN NOISE OUTSIDE\n",
    "    difference1 = abs(acc - acc_last)\n",
    "    difference2 = abs(acc_last - acc_prev)\n",
    "    if (difference1 < percentile * Gaussian_noise and difference2 < percentile * Gaussian_noise):\n",
    "        if(dbg2 or dbg18): \n",
    "            print(\"no improvement, discard last batch\")\n",
    "            print(\"differnece1 = \",difference1,\", difference2 = \",difference2)\n",
    "            if(dbg18): set_trace() #################\n",
    "        return 0   # no improvement, discard last batch\n",
    "    else:\n",
    "        if(dbg2 or dbg18): \n",
    "            print(\"improvement -- move last batch to new baseBatch\")\n",
    "            print(\"differnece1 = \",difference1,\", difference2 = \",difference2)\n",
    "        return 1   # improvement -- move last batch to new baseBatch\n",
    "\n",
    "\n",
    "#========================= MAIN =======================\n",
    "if (train):\n",
    "    '''It may help to have an outline. In the training loops,\n",
    "    j1-loop makes baseBatche's which are then copied to 3 incremental batch's.\n",
    "    After the loop there are predicted steps in best directions of each of\n",
    "    the 3 PID parameters individually as the k-loop runs 0,..,2.\n",
    "    This  is a crude way of estimating a gradient.\n",
    "    In the i-loop, the three steps are added to the parameters, in K,\n",
    "    for a new baseBatch for the next itereation.\n",
    "    The NN is then run on this batch, which is split into training and test\n",
    "    files equally.\n",
    "    This may show an \"improvement\", in which case the baseBatch is kept, else discarded.\n",
    "    outer n-loop continues.  We consider breaking if no improvement, but here the\n",
    "    n-loop just selects a next batch of 20 rows out of the batch_0 file, which has 100.\n",
    "\n",
    "    Kp = 1.2  # proportional gain (g kg-1 min-1) mmHg-1,  Might be as low of 0.3.\n",
    "    Ki = 0.8  # integral gain\n",
    "    Kd = 0.1  # differential gain\n",
    "    '''\n",
    "    K = [Kp, Ki, Kd]  # initial parameters for PID and NN\n",
    "    Gaussian = 0.0    # Kalman estimate of the distribution of NN accuracies\n",
    "    trainingRows = []\n",
    "    finalMAP = initialMAP\n",
    "    \n",
    "    #Here we  read the training file, of 100 rows, in \"batches\" of 20,\n",
    "    #giving dataSize / 20  training cycles.\n",
    "    training = []\n",
    "    # Read entire training file, selecting batches from it.\n",
    "    # Actually there is only one row, of length, dataSize.\n",
    "    success = 1\n",
    "    try:\n",
    "        dataName = \"RNN_training.csv\"\n",
    "        with open(dataName, 'r', newline='', encoding=\"utf8\") as inp:\n",
    "            csv_in = csv.reader(inp, delimiter=',')\n",
    "            count = 0\n",
    "            for row in csv_in:\n",
    "                trainingRows.append(row)\n",
    "                rowS = len(row)\n",
    "                count += 1\n",
    "                for i2 in range(0,rowS):\n",
    "                    training.append(row[i2])\n",
    "        if(dbg7): print(\"MAIN: input fileSize = \",count)\n",
    "        dataSize = count\n",
    "    except OSError as err:\n",
    "        print(\"OS error: {0}\".format(err))\n",
    "        print(print_frame())\n",
    "        success = 2\n",
    "        raise \n",
    "    except ValueError:\n",
    "        print(\"Could not convert data to an integer.\")\n",
    "        print(print_frame())\n",
    "        success = 3\n",
    "        raise \n",
    "    except IOError:\n",
    "        print (\"File error with :\", dataName)\n",
    "        print(print_frame())\n",
    "        success = 4\n",
    "        raise \n",
    "    except:\n",
    "        print(\"Unexpected error: \", sys.exc_info()[0])\n",
    "        print(print_frame())\n",
    "        success = 5\n",
    "        raise \n",
    "    finally:\n",
    "        inp.close()\n",
    "        if(dbg7):\n",
    "            print(\"MAIN: data file entry, success = \",success)\n",
    "        if (success != 1):\n",
    "            set_trace() ###########  BAD DATA ENTRY\n",
    "        \n",
    "    # Note: instead of this n1 loop, we could run through a list of training batchs\n",
    "    # from other sources such as NoisySinusoids or Glitches.\n",
    "    for n1 in range(0, trainingCycles):   #  Run training sets through PID-NN\n",
    "        batchRows = []\n",
    "        # Peel out the section of training file from M*n to M*(n+1) as a baseBatch.\n",
    "        # In n-th loop peel off the batch from M * n to M * (n+1)\n",
    "        start = n1 * timeSteps\n",
    "        endit = (n1+1) * timeSteps\n",
    "        stopit = 0\n",
    "        begin = 0\n",
    "        count = 0\n",
    "        print(\"--- start = \",start,\", endit = \",endit)\n",
    "\n",
    "        for peel in range(0, M):\n",
    "            begin = start + peel\n",
    "            stopit = endit + peel\n",
    "            batchRows.append(training[begin:stopit])\n",
    "        if(dbg3): set_trace() ###################  \n",
    "        baseBatch = fileRoot+str(n1)+\".csv\"   # base batch for n1-th training cycle\n",
    "        # write baseBatch to be read later and parms varied in \"deltaBatch\"es\n",
    "        try:\n",
    "            with open(baseBatch, 'w', newline ='', encoding = \"utf8\") as outp:\n",
    "                csv_out = csv.writer(outp, delimiter=',')\n",
    "                for k in range(0, M):\n",
    "                    csv_out.writerow(batchRows[k])\n",
    "        except OSError as err:\n",
    "            print(\"OS error : {0}\".format(err))\n",
    "            print(print_frame())\n",
    "            success = 0\n",
    "        except ValueError:\n",
    "            print(\"Could not convert data to an integer.\")\n",
    "            print(print_frame())\n",
    "            success = 0\n",
    "        except IOError:\n",
    "            print (\"File error with :\", baseBatch)\n",
    "            print(print_frame())\n",
    "            success = 0\n",
    "        except:\n",
    "            print(\"Unexpected error : \", sys.exc_info()[0])\n",
    "            success = 0\n",
    "            print(print_frame())\n",
    "        finally:\n",
    "            outp.close()\n",
    "            if(dbg7): print(\"MAIN: Successfuly wrote base batch \",baseBatch)\n",
    "               \n",
    "        for i in range(0, batchSize):   #  Run varied batches through PID-NN\n",
    "            if(dbg3): set_trace() ########################\n",
    "            if (i == 0):\n",
    "                baseBatch = fileRoot+str(n1)+\".csv\"\n",
    "                newBatch = baseBatch\n",
    "            else:\n",
    "                baseBatch = newBaseBatch   # returned from end of this loop\n",
    "                newBatch = baseBatch\n",
    "                \n",
    "            file = open(newBatch)\n",
    "            lengF = len(file.readlines())\n",
    "            if (batchSize != lengF):\n",
    "                    print(\"*** newBatch size wrong = \",lengF,\" at i = \",i,\", n = \",n1)\n",
    "                    print(print_frame())\n",
    "                    break\n",
    "            if(dbg7): \n",
    "                print(\"--- At beginning looping over baseBatch = \",baseBatch)\n",
    "                print(\"    batchSize = \",batchSize)\n",
    "            \n",
    "            #--Inner loop creates rows of batch_i_j as small steps in each component of K.\n",
    "            #  The PID parameter array, K, will be collected after the end of the loop.\n",
    "            #  Run Driver_NN_PID_setup to get initial \"_0\" batch.\n",
    "\n",
    "            average_row_error = 0\n",
    "            ave_score = 0\n",
    "            scores = [0 for i in range(0,batchSize)]\n",
    "            acc_prev = 0\n",
    "            acc_last = 0\n",
    "\n",
    "            # This loop varies the j1-th coordinate of K to create deltaBatch, and\n",
    "            # runs this file with PID, collecting a score base on how close to MAP target\n",
    "            # the PID gets at the end of the profile (batch row).\n",
    "\n",
    "            for j1 in range(0,2):  # runs PID with variations in each of 3 PID parameters\n",
    "\n",
    "                # Change the j1-th coordinate in the best gradient direction\n",
    "                delta_K = step_j_th_parm (accuracy, acc_last, j1, K, epsilon)\n",
    "                \n",
    "                # Fill in the j-th coordinate of params, K, to create a \"delta\"\n",
    "                deltaBatch = fileRoot+str(i)+\"_\"+str(j1)+\".csv\"\n",
    "\n",
    "                success = create_batch_i_j (n1, i, j1, baseBatch, deltaBatch, delta_K, noise)\n",
    "                if(dbg7): \n",
    "                    print(\"-Main: create_batch_i_j baseBatch = \",baseBatch,\", deltaBatch = \",deltaBatch,\", success = \",success)\n",
    "                if (success != 1):\n",
    "                    break\n",
    "                if(dbg7): set_trace() ################## Passed the early break in j1 - loop\n",
    "                if(dbg): \n",
    "                    print(\"- j1 loop new : \",deltaBatch,\" at i = \",i,\", j = \",j1,\", n = \",n1)\n",
    "                    print(\"  batchSize = \",batchSize)\n",
    "                # Read the new \"batch\" and run PID on each measure in each row,\n",
    "                # computing a score from the final MAP reached by the PID.\n",
    "                if(dbg7): set_trace() #######################\n",
    "                try:\n",
    "                    rows = []\n",
    "                    count = 0\n",
    "                    with open(deltaBatch, 'r', newline='', encoding=\"utf8\") as inp:\n",
    "                        csv_in = csv.reader(inp, delimiter=',')\n",
    "                        for row in csv_in:\n",
    "                            rows.append(row)\n",
    "                            count += 1\n",
    "                    if (batchSize != count):\n",
    "                        print(\"*** MAIN: Wrong deltaBatch file size \",count)\n",
    "                        print(print_frame())\n",
    "                        if(dbg7): set_trace() #######################\n",
    "\n",
    "                    for k in range(0,batchSize):\n",
    "                        # For each row of batch_i_j, advance PID across simulated measurements\n",
    "                        row = rows[k]\n",
    "                        row_size = len(row)\n",
    "                        if (row_size > rowSize):\n",
    "                            if(dbg): print(\"---Some long rows in k-loop: \",row_size)\n",
    "                            row_size = rowSize\n",
    "                            if(dbg): print(\"---Resetting row_size = : \",row_size)\n",
    "                        rowLength = len(row)\n",
    "                        if(dbg1): print(\"MAIN: before PID, rowLength = \",rowLength,\", timeSteps = \",timeSteps)\n",
    "\n",
    "                        mp = 0\n",
    "\n",
    "                        if(dbg12): set_trace() ###################### step through\n",
    "                        # Run PID et al. across the k-th row\n",
    "                        for m in range(0,rowSize):\n",
    "                            owr = [ast.literal_eval(ep) for ep in row]\n",
    "                            if(dbg): print(\"MAIN: Inner loop m \",m,\" entering PID\")\n",
    "                            pid = PID (i,j1,k,m,\"bleeding.csv\",owr,\"controls.csv\",finalMAP,VB=2500,K0=2.1,alpha=0.5)\n",
    "                            pid.run_model()\n",
    "                            pid.advance_PID()   # value reached at end of row test\n",
    "                            mp = pid.finalMAP\n",
    "                            if(dbg): print(\"PID's finalMAP = \",mp)\n",
    "                        if(dbg12): set_trace() ################################ end m-loop\n",
    "\n",
    "                        scores[k] = min( 0.99, 1 - abs((mp - targetMAP) / targetMAP))\n",
    "                        ave_score += scores[k]\n",
    "                    if(dbg2):\n",
    "                        print(\"rowSize = \",rowSize,\", row_size = \",row_size,\" at j1 = \",j1,\", k = \",k)\n",
    "                        set_trace() #################### row_size, rowSize\n",
    "                except OSError as err:\n",
    "                    print(\"OS error : {0}\".format(err))\n",
    "                    print(print_frame())\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    print(\"Could not convert data to an integer.\")\n",
    "                    print(print_frame())\n",
    "                    break\n",
    "                except IOError:\n",
    "                    print (\"File error with : \", baseBatch)\n",
    "                    print(print_frame())\n",
    "                    break\n",
    "                except:\n",
    "                    print(\"Unexpected error in maain:\", sys.exc_info()[0])\n",
    "                    print(print_frame())\n",
    "                    break\n",
    "                    raise\n",
    "                finally:\n",
    "                    inp.close()\n",
    "                    if(dbg2):\n",
    "                        print(\"deltaBatch \",deltaBatch,\" i = \",i,\", j = \",j1,\", scores = \",scores)\n",
    "                        print(\"  K[\",j1,\"] = \",K[j1])\n",
    "                #--end j1 loop\n",
    "                \n",
    "\n",
    "                ave_score = ave_score / 3\n",
    "                \n",
    "                # sync up named parameters with K array\n",
    "                Kp = K[0]\n",
    "                Ki = K[1]\n",
    "                Kd = K[2]\n",
    "\n",
    "                if(dbg2 or dbg18):\n",
    "                    print(\"Finished j1 and inner loops.  Now K = \",K,\", ave_score = \",ave_score)\n",
    "\n",
    "                # create new batch for NN based on best K's incremented in gradient directions\n",
    "                newBatch = fileRoot+str(i+1)+\".csv\"\n",
    "                create_batch_i(n1, i+1, baseBatch, newBatch, K, scores, noise)\n",
    "                # Next iteration of i-loop should pick the new baseBatch up.\n",
    "                \n",
    "                if(dbg3 or dbg18): set_trace() #####################\n",
    "                # step forward for accuracy parameters\n",
    "                acc_prev = acc_last\n",
    "                acc_last = accuracy\n",
    "                accuracy = ave_score\n",
    "                \n",
    "                # THIS accuracy SETTING NEEDS TO BE CHANGED FOR EACH CONFIGURATION:\n",
    "                # Accuracy can be set be either controller or neural net.  If\n",
    "                # neural net is not configured, the default will be the aveScore as below:\n",
    "\n",
    "                if(dbg or dbg18): \n",
    "                    print(\"--- End of Loops i = \",i,\", n = \",n1)\n",
    "                    print(\"   accuracy = \",accuracy)\n",
    "                    print(\"   acc_last = \",acc_last)\n",
    "                    print(\"   acc_prev = \",acc_prev)\n",
    "                    print(\"   ave_score = \",ave_score)\n",
    "                    print(\" \")\n",
    "\n",
    "                # Re-train NN on the revised i-th batch\n",
    "                if(dbg3): set_trace() ############ check numbers above\n",
    "                if(dbg): print(\"Near NN, newBatch = \",newBatch)\n",
    "\n",
    "                file = open(newBatch)\n",
    "                lengF = len(file.readlines())\n",
    "                #if(dbg8): set_trace() ###################### lengF   starts NN \n",
    "                if (batchSize < lengF):\n",
    "                    print(\"*** newBatch is short : \",lengF,\" Not running NN\")\n",
    "                else:\n",
    "                    if(cutNeurons):\n",
    "                        loss = 0\n",
    "                        rmse = 0\n",
    "                    else:\n",
    "                        print(\"--- Starting NeuralNet --- \")\n",
    "\n",
    "                        loss, rmse = NeuralNet(training)\n",
    "                        accuracy = rmse   #in this case\n",
    "\n",
    "                        if(dbg8): set_trace() ###################### ends NN \n",
    "                        '''Loss function measures the difference between the predicted label and the ground truth label. \n",
    "                        E.g., square loss is  L(y^,y)=(y^−y)^2 , hinge loss is  L(y^,y)=max{0,1 − y^ x y} ...'''\n",
    "                        if (dbg):\n",
    "                            print(\"--NN RESULTS:\")\n",
    "                            print(\"    loss = \",loss,\", rmse = \",rmse)\n",
    "                            print(\"    After NN call Kp = \",K[0],\", Ki = \",K[1],\", Kd = \",K[2])\n",
    "                            print(print_frame())\n",
    "                # Estimate presumed Gaussian noise of the NN's accuracy\n",
    "                if (i == 0):\n",
    "                    W1 = 1\n",
    "                    W2 = 0\n",
    "                else:\n",
    "                    W1 = 1 / float(i)\n",
    "                    W2 = 1 - W1\n",
    "                Gaussian = W1 * accuracy + W2 * Gaussian\n",
    "                if(dbg1): print(\"Gaussian noise level is \",Gaussian)\n",
    "\n",
    "            #-- end j1-Loop over batch and delta batches\n",
    "\n",
    "            improve = improvement(accuracy, acc_last, acc_prev, Gaussian)\n",
    "            \n",
    "            if(dbg5): set_trace()  #############################\n",
    "            if (improve == 0): \n",
    "                print(\"-- No significant impovement in accuracy. discarding newBatch\")\n",
    "                file = open(baseBatch)\n",
    "                lengF = len(file.readlines())\n",
    "                if (batchSize != lengF):\n",
    "                    print(\"*** baseBatch is wrong length : \",lengF, \" Breaking i-loop\")\n",
    "                    print(print_frame())\n",
    "                    break\n",
    "                else:\n",
    "                    newBaseBatch = baseBatch\n",
    "            else: # replace old baseBatch with last batch from PID\n",
    "                file = open(newBatch)\n",
    "                lengF = len(file.readlines())\n",
    "                if (batchSize != lengF):\n",
    "                    print(\"*** newBatch is wrong length : \",lengF, \" Breaking i-loop\")\n",
    "                    print(print_frame())\n",
    "                    break\n",
    "                else:\n",
    "                    newBaseBatch = newBatch\n",
    "            print(\"--- At end of batch loops, newBaseBatch = \",newBaseBatch)\n",
    "        #--end batch i-loop\n",
    "    #--end training n-loop\n",
    "#=========================================================\n",
    "if (test):\n",
    "    if(dbg): print(\"========== STARTING VALIDATION =========\")\n",
    "    '''Validation test\n",
    "    Read line from RNN_training.csv data file;\n",
    "    run PID  which may input a fluid losses file as an option;\n",
    "    run pump simulator (basically a linear transfer function with lag);\n",
    "    write line to control file;\n",
    "    run \"model\" of body responses;\n",
    "    compute error between body response and desired target MAP;\n",
    "        \n",
    "    As prerequisite, this depends on the parameters, K, being adequately trained\n",
    "    by the training loop.  Keeping its trained node weights, the NN now serves\n",
    "    to make a small correction to the PID output.\n",
    "    \n",
    "    Run NN to make a correction to PID output (trained separately for that as above);\n",
    "    use accuracy to adjust step size (and other search parameter as needed).'''\n",
    "    ABC = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
